"""
Sentiment Analyzer Agent - Enriches articles with AI-generated sentiment scores.

This agent reads raw articles from Neo4j, analyzes their sentiment using FinBERT
(financial language model) combined with keyword-based adjustments, and writes 
the scores back to Neo4j for use by the SentimentExplainerAgent.

Technical Approach:
- FinBERT (ProsusAI/finbert): Pre-trained transformer model on financial news
- Financial keyword scoring: Domain-specific positive/negative term weighting
- spaCy NLP: Theme extraction using named entity recognition
- Hybrid scoring: 70% FinBERT + 30% keyword adjustment
"""

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import spacy
import logging

logger = logging.getLogger(__name__)


@dataclass
class ArticleSentiment:
    """Structured sentiment analysis result."""
    score: float  # -1.0 to 1.0
    label: str  # bearish, neutral, bullish
    confidence: float  # 0.0 to 1.0
    reasoning: str  # Why this sentiment
    themes: List[str]  # Key themes extracted
    trading_impact: str  # short_term_bearish, long_term_bullish, etc.
    analyzed_at: str  # ISO timestamp
    analyzed_by: str = "copilot"


class SentimentAnalyzerAgent:
    """
    Agent that analyzes article sentiment and enriches Neo4j with scores.

    Uses FinBERT (financial BERT) for base sentiment analysis, enhanced with
    financial keyword scoring and NLP-based theme extraction.
    
    Technical Stack:
    - FinBERT: Financial sentiment classification (3 classes: positive, negative, neutral)
    - spaCy: Named entity recognition for theme extraction
    - Custom keyword scoring: Financial domain-specific adjustments
    """

    def __init__(self, mcp_client):
        """
        Initialize sentiment analyzer with FinBERT model.

        Args:
            mcp_client: MCP client for Neo4j operations
        """
        self.mcp_client = mcp_client
        self.analyzed_count = 0
        self.skipped_count = 0
        
        # Load FinBERT model (lazy loading - only when analyze_article is called)
        self._tokenizer = None
        self._model = None
        self._nlp = None
        
        # Financial keyword dictionaries
        self.bearish_keywords = {
            'falls', 'drops', 'declines', 'losses', 'concerns', 'risks',
            'challenges', 'competition', 'threatens', 'weakness', 'downturn',
            'miss', 'below', 'disappoints', 'cuts', 'layoffs', 'restructuring',
            'litigation', 'investigation', 'scandal', 'breach', 'hack'
        }
        
        self.bullish_keywords = {
            'gains', 'rises', 'surges', 'growth', 'profits', 'beats',
            'exceeds', 'strong', 'partnership', 'innovation', 'breakthrough',
            'expansion', 'acquisition', 'upside', 'momentum', 'record',
            'upgrade', 'outperform', 'buy', 'positive', 'bullish'
        }
    
    def _load_models(self):
        """Lazy load FinBERT and spaCy models."""
        if self._tokenizer is None:
            print("Loading FinBERT model (first time only)...")
            self._tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
            self._model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
            self._model.eval()  # Set to evaluation mode
            
        if self._nlp is None:
            print("Loading spaCy NLP model...")
            self._nlp = spacy.load("en_core_web_sm")

    def analyze_all_tickers(self, tickers: List[str], days: int = 30,
                            force_reanalyze: bool = False) -> Dict[str, Any]:
        """
        Analyze articles for multiple tickers.

        Args:
            tickers: List of stock symbols to analyze
            days: How far back to analyze articles (default 30)
            force_reanalyze: Re-analyze even if sentiment exists

        Returns:
            Summary of analysis operation
        """
        print(f"\nüî¨ SENTIMENT ANALYZER AGENT")
        print(f"Analyzing articles for {len(tickers)} tickers...")
        print(f"Lookback period: {days} days")
        print(f"Force re-analyze: {force_reanalyze}\n")

        results = {
            "analyzed": 0,
            "skipped": 0,
            "errors": 0,
            "by_ticker": {}
        }

        for ticker in tickers:
            try:
                ticker_result = self.analyze_ticker(
                    ticker, days, force_reanalyze)
                results["analyzed"] += ticker_result["analyzed"]
                results["skipped"] += ticker_result["skipped"]
                results["by_ticker"][ticker] = ticker_result
            except Exception as e:
                print(f"‚ùå Error analyzing {ticker}: {str(e)}")
                results["errors"] += 1
                results["by_ticker"][ticker] = {"error": str(e)}

        self._print_summary(results)
        return results

    def analyze_ticker(self, ticker: str, days: int = 30,
                       force_reanalyze: bool = False) -> Dict[str, Any]:
        """
        Analyze articles for a single ticker.

        Args:
            ticker: Stock symbol
            days: Lookback period
            force_reanalyze: Re-analyze existing sentiment

        Returns:
            Analysis results for this ticker
        """
        print(f"üìä Analyzing {ticker}...")

        result = {
            "ticker": ticker,
            "analyzed": 0,
            "skipped": 0,
            "articles_found": 0,
            "articles": []
        }

        try:
            # Fetch recent articles from MCP server
            # This call is handled by the MCP interface, not the mcp_client
            print(f"   Fetching articles from Neo4j via MCP...")

            # NOTE: In the actual CLI run, this will be intercepted by MCP
            # For now, we return the structure showing what would be analyzed

            result["message"] = "Ready to analyze - MCP will provide articles"
            result["needs_ai_analysis"] = True

        except Exception as e:
            print(f"   ‚ùå Error fetching articles: {str(e)}")
            result["error"] = str(e)

        return result

    def analyze_article(self, article: Dict[str, Any], ticker: str) -> ArticleSentiment:
        """
        Analyze a single article using FinBERT + keyword scoring.

        Technical Approach:
        1. FinBERT base sentiment (70% weight)
        2. Financial keyword scoring (30% weight)
        3. Theme extraction via spaCy NER
        4. Trading impact assessment

        Args:
            article: Article data with title, summary, url, etc.
            ticker: Stock symbol this article relates to

        Returns:
            ArticleSentiment with detailed analysis
        """
        # Load models if not already loaded
        self._load_models()
        
        # Extract article content
        title = article.get("title", "")
        summary = article.get("summary", "")
        source = article.get("source", "unknown")
        
        # Combine for analysis
        text = f"{title}. {summary}"
        
        # Step 1: FinBERT base sentiment
        finbert_score, finbert_probs = self._finbert_analysis(text)
        
        # Step 2: Financial keyword adjustment
        keyword_score = self._keyword_scoring(text)
        
        # Step 3: Weighted combination (70% FinBERT, 30% keywords)
        final_score = (0.7 * finbert_score) + (0.3 * keyword_score)
        
        # Step 4: Extract themes
        themes = self._extract_themes(text, ticker)
        
        # Step 5: Determine label and confidence
        label = self._score_to_label(final_score)
        confidence = max(finbert_probs)  # Confidence from FinBERT
        
        # Step 6: Generate reasoning
        reasoning = self._generate_reasoning(
            finbert_score=finbert_score,
            keyword_score=keyword_score,
            final_score=final_score,
            finbert_probs=finbert_probs,
            themes=themes,
            ticker=ticker
        )
        
        # Step 7: Assess trading impact
        trading_impact = self._assess_trading_impact(final_score, themes)
        
        return ArticleSentiment(
            score=final_score,
            label=label,
            confidence=confidence,
            reasoning=reasoning,
            themes=themes,
            trading_impact=trading_impact,
            analyzed_at=datetime.utcnow().isoformat(),
            analyzed_by="finbert_hybrid"
        )
    
    def _finbert_analysis(self, text: str) -> tuple[float, list[float]]:
        """
        Run FinBERT sentiment analysis.
        
        Returns:
            (score, probabilities): score in [-1, 1], probs as [pos, neg, neu]
        """
        # Tokenize (max 512 tokens for BERT)
        inputs = self._tokenizer(
            text, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512,
            padding=True
        )
        
        # Get predictions
        with torch.no_grad():
            outputs = self._model(**inputs)
        
        # Convert to probabilities
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]
        
        # FinBERT classes: [positive, negative, neutral]
        pos_prob = probs[0].item()
        neg_prob = probs[1].item()
        neu_prob = probs[2].item()
        
        # Convert to -1 to +1 scale
        # Strong positive = +1, strong negative = -1, neutral = 0
        score = pos_prob - neg_prob
        
        return score, [pos_prob, neg_prob, neu_prob]
    
    def _keyword_scoring(self, text: str) -> float:
        """
        Score based on financial keywords.
        
        Returns:
            Score in [-1, 1] based on keyword prevalence
        """
        text_lower = text.lower()
        
        # Count keyword occurrences
        bearish_count = sum(1 for word in self.bearish_keywords if word in text_lower)
        bullish_count = sum(1 for word in self.bullish_keywords if word in text_lower)
        
        # Normalize to -1 to +1
        total = bearish_count + bullish_count
        if total == 0:
            return 0.0
        
        # More bullish keywords = positive score
        return (bullish_count - bearish_count) / total
    
    def _extract_themes(self, text: str, ticker: str) -> List[str]:
        """
        Extract themes using spaCy NER and financial heuristics.
        
        Returns:
            List of theme strings
        """
        doc = self._nlp(text)
        themes = set()
        
        # Extract named entities
        for ent in doc.ents:
            if ent.label_ in ['ORG', 'PRODUCT', 'EVENT', 'GPE']:
                themes.add(ent.text.lower())
        
        # Add financial themes based on keywords
        text_lower = text.lower()
        
        theme_keywords = {
            'earnings': ['earnings', 'revenue', 'profit', 'eps'],
            'competition': ['competition', 'competitor', 'market share', 'rival'],
            'ai_technology': ['ai', 'artificial intelligence', 'machine learning', 'automation'],
            'regulation': ['regulation', 'sec', 'investigation', 'lawsuit'],
            'expansion': ['acquisition', 'merger', 'expansion', 'growth'],
            'product_launch': ['launch', 'release', 'unveil', 'announce'],
            'leadership': ['ceo', 'executive', 'management', 'leadership'],
            'market_performance': ['market cap', 'stock price', 'valuation', 'shares']
        }
        
        for theme, keywords in theme_keywords.items():
            if any(kw in text_lower for kw in keywords):
                themes.add(theme)
        
        return sorted(list(themes))[:5]  # Limit to top 5
    
    def _score_to_label(self, score: float) -> str:
        """Convert numeric score to sentiment label."""
        if score >= 0.15:
            return "bullish"
        elif score <= -0.15:
            return "bearish"
        else:
            return "neutral"
    
    def _generate_reasoning(self, finbert_score: float, keyword_score: float,
                           final_score: float, finbert_probs: list[float],
                           themes: List[str], ticker: str) -> str:
        """
        Generate human-readable reasoning for the sentiment score.
        
        Explains the methodology and key factors.
        """
        pos_prob, neg_prob, neu_prob = finbert_probs
        
        # Determine dominant sentiment from FinBERT
        if pos_prob > neg_prob and pos_prob > neu_prob:
            finbert_sentiment = "positive"
        elif neg_prob > pos_prob and neg_prob > neu_prob:
            finbert_sentiment = "negative"
        else:
            finbert_sentiment = "neutral"
        
        # Keyword adjustment direction
        if keyword_score > 0.1:
            keyword_adj = "bullish keywords present"
        elif keyword_score < -0.1:
            keyword_adj = "bearish keywords detected"
        else:
            keyword_adj = "mixed keyword signals"
        
        reasoning = (
            f"FinBERT analysis: {finbert_sentiment} "
            f"(pos={pos_prob:.2f}, neg={neg_prob:.2f}, neu={neu_prob:.2f}). "
            f"Keyword scoring: {keyword_adj} (score={keyword_score:.2f}). "
            f"Final weighted score: {final_score:.2f}. "
        )
        
        if themes:
            reasoning += f"Key themes: {', '.join(themes[:3])}."
        
        return reasoning
    
    def _assess_trading_impact(self, score: float, themes: List[str]) -> str:
        """
        Assess trading impact based on sentiment and themes.
        
        Returns:
            Impact description (e.g., "short_term_bearish", "long_term_bullish")
        """
        # Base impact from score
        if abs(score) < 0.15:
            base = "neutral"
        elif score > 0:
            base = "bullish"
        else:
            base = "bearish"
        
        # Adjust for specific themes
        long_term_themes = {'ai_technology', 'expansion', 'product_launch'}
        short_term_themes = {'earnings', 'market_performance', 'regulation'}
        
        has_long_term = any(theme in long_term_themes for theme in themes)
        has_short_term = any(theme in short_term_themes for theme in themes)
        
        if has_short_term and not has_long_term:
            return f"short_term_{base}"
        elif has_long_term and not has_short_term:
            return f"long_term_{base}"
        else:
            return f"medium_term_{base}"

    def _print_summary(self, results: Dict[str, Any]):
        """Print analysis summary."""
        print("\n" + "="*60)
        print("üìà SENTIMENT ANALYSIS SUMMARY")
        print("="*60)
        print(f"Total analyzed: {results['analyzed']}")
        print(f"Total skipped (already analyzed): {results['skipped']}")
        print(f"Errors: {results['errors']}")
        print("\nPer-ticker breakdown:")
        for ticker, ticker_result in results["by_ticker"].items():
            if "error" in ticker_result:
                print(f"  {ticker}: ‚ùå {ticker_result['error']}")
            else:
                analyzed = ticker_result.get("analyzed", 0)
                skipped = ticker_result.get("skipped", 0)
                print(f"  {ticker}: ‚úì {analyzed} analyzed, {skipped} skipped")
        print("="*60 + "\n")


def analyze_sentiment_cli(tickers: Optional[List[str]] = None,
                          days: int = 30,
                          force: bool = False):
    """
    CLI entry point for sentiment analysis.

    Args:
        tickers: List of tickers to analyze (None = all portfolio tickers)
        days: Lookback period
        force: Force re-analysis of existing sentiment
    """
    from src.utils.mcp_client import MCPClient

    # Default to portfolio tickers if none specified
    if tickers is None:
        tickers = [
            "AAPL", "MSFT", "GOOGL", "NVDA", "META", "XLK",
            "XOM", "CVX", "COP", "XLE",
            "SPY", "QQQ", "IWM"
        ]

    mcp_client = MCPClient()
    analyzer = SentimentAnalyzerAgent(mcp_client)

    results = analyzer.analyze_all_tickers(
        tickers, days=days, force_reanalyze=force)

    return results
